{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charisliao/crisis-text-line-detection/blob/master/CrisisTextLineDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8xZQatA1BN_"
      },
      "source": [
        "# Hello! Welcome to Crsis Text Line Detection Notebook.\n",
        "\n",
        "### Author: Charis Liao\n",
        "\n",
        "I aim to build an AI/data system to help identify crisis text lines in the hope the spread mental health awareness. The input text will be ranked between 0 and 1. The closer the text is to 0, the safer (positive) it is , and 1 otherwise.\n",
        "\n",
        "This notebook handles the back-end AI modeling for Anvil (the front-end interface for crisis text line detection).\n",
        "\n",
        "For the project, I will:\n",
        "\n",
        "- Create a working database table that stores user input from user interface using Anvil.     \n",
        "- create a working GBDT model     \n",
        "  - Pre-process data (vectorize text to matrix of token counts, model, then post-process data (vectorize text to matrix of token counts; parse model result to pass to Anvil)\n",
        "- Connect Colab notebook and Anvil app via UpLink"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the required modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import xgboost\n",
        "\n",
        "from scipy.sparse import hstack, vstack # use hstack to concatenate features horizontally\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# set option below so Pandas dataframe can output readable text, not truncated\n",
        "pd.set_option('display.max_colwidth', 0)\n",
        "\n",
        "# Install the latest version of gdown\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "# download the file\n",
        "#!gdown --id 1-3nuUR2kRu7OIIEw2Q_tQ1BGOSZRkkNt\n",
        "!gdown --id 1lBXN_DLCnWpTOHljHPoPwY_bt3WD2HsR\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyNvxdvLXwNp",
        "outputId": "8885c74d-41e6-4479-c7a2-a999845c5fc3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "Successfully installed gdown-4.7.1\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lBXN_DLCnWpTOHljHPoPwY_bt3WD2HsR\n",
            "To: /content/Tweets.csv\n",
            "100% 473k/473k [00:00<00:00, 5.09MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "176A0_2zF8gM"
      },
      "outputs": [],
      "source": [
        "# read datasets\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/tony51307/datax-gsi/main/train.csv', on_bad_lines='skip',encoding='unicode_escape')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/tony51307/datax-gsi/main/test.csv', on_bad_lines='skip')\n",
        "train.dropna(inplace = True)\n",
        "x_train, y_train, x_test, y_test = train['text'], train['label'], test['text'], test['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AKj_Au8sI1a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23fbf8f-c960-4495-fcc8-60839def5600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4991,) (4991,)\n",
            "(999,) (999,)\n",
            "       able  absolutely  account  actually  advice  afford  afraid  age  ago  \\\n",
            "0  0.000000  0.0         0.0      0.000000  0.0     0.0     0.0     0.0  0.0   \n",
            "1  0.000000  0.0         0.0      0.165595  0.0     0.0     0.0     0.0  0.0   \n",
            "2  0.165124  0.0         0.0      0.000000  0.0     0.0     0.0     0.0  0.0   \n",
            "3  0.000000  0.0         0.0      0.000000  0.0     0.0     0.0     0.0  0.0   \n",
            "4  0.000000  0.0         0.0      0.000000  0.0     0.0     0.0     0.0  0.0   \n",
            "\n",
            "   alcohol  ...  wrong   xb  yeah  year  years  yes  yesterday  young  youã  \\\n",
            "0  0.0      ...  0.0    0.0  0.0   0.0   0.0    0.0  0.0        0.0    0.0    \n",
            "1  0.0      ...  0.0    0.0  0.0   0.0   0.0    0.0  0.0        0.0    0.0    \n",
            "2  0.0      ...  0.0    0.0  0.0   0.0   0.0    0.0  0.0        0.0    0.0    \n",
            "3  0.0      ...  0.0    0.0  0.0   0.0   0.0    0.0  0.0        0.0    0.0    \n",
            "4  0.0      ...  0.0    0.0  0.0   0.0   0.0    0.0  0.0        0.0    0.0    \n",
            "\n",
            "   âªã  \n",
            "0  0.0  \n",
            "1  0.0  \n",
            "2  0.0  \n",
            "3  0.0  \n",
            "4  0.0  \n",
            "\n",
            "[5 rows x 500 columns]\n",
            "   able  absolutely  account  actually  advice  afford    afraid       age  \\\n",
            "0  0.0   0.0         0.0      0.0       0.0     0.0     0.000000  0.000000   \n",
            "1  0.0   0.0         0.0      0.0       0.0     0.0     0.000000  0.000000   \n",
            "2  0.0   0.0         0.0      0.0       0.0     0.0     0.000000  0.000000   \n",
            "3  0.0   0.0         0.0      0.0       0.0     0.0     0.000000  0.000000   \n",
            "4  0.0   0.0         0.0      0.0       0.0     0.0     0.145867  0.147218   \n",
            "\n",
            "   ago  alcohol  ...  wrong   xb  yeah  year  years  yes  yesterday  young  \\\n",
            "0  0.0  0.0      ...  0.0    0.0  0.0   0.0   0.0    0.0  0.0        0.0     \n",
            "1  0.0  0.0      ...  0.0    0.0  0.0   0.0   0.0    0.0  0.0        0.0     \n",
            "2  0.0  0.0      ...  0.0    0.0  0.0   0.0   0.0    0.0  0.0        0.0     \n",
            "3  0.0  0.0      ...  0.0    0.0  0.0   0.0   0.0    0.0  0.0        0.0     \n",
            "4  0.0  0.0      ...  0.0    0.0  0.0   0.0   0.0    0.0  0.0        0.0     \n",
            "\n",
            "   youã  âªã  \n",
            "0  0.0   0.0  \n",
            "1  0.0   0.0  \n",
            "2  0.0   0.0  \n",
            "3  0.0   0.0  \n",
            "4  0.0   0.0  \n",
            "\n",
            "[5 rows x 500 columns]\n"
          ]
        }
      ],
      "source": [
        "# preprocess the data\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def first_preprocessor(s):\n",
        "   #convert to lowercase (which CountVectorizer and TfidfVectorizer do by default)\n",
        "    s = s.lower()\n",
        "\n",
        "    # replace \"&amp\" with \"and\"\n",
        "    s = s.replace(\"&amp\", \"and\")\n",
        "\n",
        "    # remove select punctation; re refers to the Regular Expression module\n",
        "    s = re.sub(\"[@,.!?:;/~*]\", \" \", s)\n",
        "\n",
        "    # replace multiple consecutive blank spaces with 1 blank space?\n",
        "    s = re.sub(\"[ ]+\", \" \", s)\n",
        "\n",
        "    # remove all numbers?\n",
        "    s = re.sub(r'\\d+', '', s)\n",
        "\n",
        "    return s\n",
        "\n",
        "# first_preprocessor(\"10 CONVERT    UPPERCASE TO LOWERCASE AND REMOVE SELECT PUNCUATION?\")\n",
        "\n",
        "ngram_range  = (1,np.random.randint(1, 3))\n",
        "stop_words   = np.random.choice([None, \"english\"])\n",
        "\n",
        "# create an instance of CountVectorizer or TfidfVectorizer\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "vectorizer   = np.random.choice([CountVectorizer(preprocessor=first_preprocessor,\n",
        "                                                 ngram_range=ngram_range,\n",
        "                                                 stop_words=stop_words,\n",
        "                                                 max_features=500),\n",
        "                                 TfidfVectorizer(preprocessor=first_preprocessor,\n",
        "                                                 ngram_range=ngram_range,\n",
        "                                                 stop_words=stop_words,\n",
        "                                                 max_features=500)\n",
        "                                ])\n",
        "\n",
        "# fit_transform a list of sentences into a matrix of token counts (e.g., word counts)\n",
        "x_train_tokenized = vectorizer.fit_transform(x_train)\n",
        "\n",
        "# combine the feature names and matrix of 1s and 0s to a dataframe\n",
        "x_train = pd.DataFrame( x_train_tokenized.toarray(),\n",
        "                        columns=vectorizer.get_feature_names_out(),\n",
        "                        index=x_train.index) # take index of original dataframe\n",
        "\n",
        "# preview the dataframe\n",
        "print(x_train.head())\n",
        "\n",
        "\n",
        "x_test_tokenized = vectorizer.transform(x_test)\n",
        "x_test = pd.DataFrame( x_test_tokenized.toarray(),\n",
        "                      columns=vectorizer.get_feature_names_out(),\n",
        "                      index=x_test.index) # take index of original dataframe\n",
        "\n",
        "# preview the dataframe\n",
        "print(x_test.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[\"num_words\"]  = np.zeros(4991)\n",
        "x_test[\"num_words\"]    = np.zeros(999)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Kpbr5xogjS1",
        "outputId": "39eb3c4d-82bf-4941-8d53-8a2409692e71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4991, 501) (4991,)\n",
            "(999, 501) (999,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GbQuTqQEI_62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430d2c0d-4655-4be9-9f6f-fb39eddf7f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.93121064 0.06878935]\n",
            " [0.3586042  0.6413958 ]\n",
            " [0.00773835 0.99226165]\n",
            " ...\n",
            " [0.00217301 0.997827  ]\n",
            " [0.05706757 0.9429324 ]\n",
            " [0.1223011  0.8776989 ]]\n",
            "\n",
            "ROC AUC SCORE: 0.9916310184033323\n"
          ]
        }
      ],
      "source": [
        "# evaluate model, use test dataset to evaluate the model on AUC score\n",
        "\n",
        "\n",
        "model = xgboost.XGBClassifier(n_estimators=100,\n",
        "                              max_depth=20,\n",
        "                              learning_rate=0.1,\n",
        "                              random_state=0)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "prediction = model.predict_proba(x_test)\n",
        "print(prediction)\n",
        "print()\n",
        "print(\"ROC AUC SCORE:\", roc_auc_score(y_test, prediction[:,1]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ROC AUC SCORE exceeds 0.99."
      ],
      "metadata": {
        "id": "WssVvrv4KsBH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_VuI_E-lZwj9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "3f5008e1-0e6b-481a-cc7d-143b5816bf2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anvil-uplink in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Collecting argparse (from anvil-uplink)\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (1.16.0)\n",
            "Requirement already satisfied: ws4py in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (0.5.1)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disconnecting from previous connection first...\n",
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket closed (code 1000, reason=b'')\n",
            "Anvil websocket open\n",
            "Connected to \"Default environment\" as SERVER\n"
          ]
        }
      ],
      "source": [
        "# connect your model to Anvil\n",
        "\n",
        "\n",
        "!pip install anvil-uplink\n",
        "import anvil.server\n",
        "\n",
        "\n",
        "\n",
        "# Connect to my anvil server\n",
        "anvil.server.connect(\"NRJUXU5RGKXINVLJFWJM6ROW-U2XEEGCOY2LICZMV\")\n",
        "\n",
        "\n",
        "@anvil.server.callable\n",
        "# this section is harder and thus written already for the in-class submission\n",
        "def sentiment(text):\n",
        "\n",
        "  # transform the input text to a vector\n",
        "  text_token = vectorizer.transform([text]).toarray()\n",
        "\n",
        "  # split the text into number of words\n",
        "  num_words = len(text.split(' '))\n",
        "\n",
        "  # append the tokenized words with the number of words\n",
        "  x = np.append(text_token, num_words)\n",
        "\n",
        "  # place the dataset in a dataframe\n",
        "  x = pd.DataFrame([x], columns=x_train.columns)\n",
        "\n",
        "  # run the predictions on that dataframe\n",
        "  prediction = model.predict_proba(x)\n",
        "\n",
        "\n",
        "  # return (label, score)\n",
        "  if prediction[0,1] >= 0.5:\n",
        "    label = \"POSITIVE\"\n",
        "    score = prediction[0,1]\n",
        "  else:\n",
        "    label = \"NEGATIVE\"\n",
        "    score = prediction[0,0]\n",
        "\n",
        "  if label == \"NEGATIVE\":\n",
        "    score = prediction[0,1]\n",
        "  elif label == \"POSITIVE\":\n",
        "    score = prediction[0,0]\n",
        "  return score\n",
        "\n",
        "  anvil.server.wait_forever()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment(\"hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cRp3sv3TttD",
        "outputId": "97e65d7d-7aa2-493a-fae3-e8007291a8ac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.055531643"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}